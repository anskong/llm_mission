{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258408cb",
   "metadata": {},
   "source": [
    "#### **[ì´ˆê¸°ê°’ ì„¤ì •]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c86328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì½ì–´ì˜¤ê¸°\n",
    "load_dotenv(override=True)  # .env íŒŒì¼ì„ ë®ì–´ì“°ê¸° ëª¨ë“œë¡œ ì½ê¸°\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "print(f\"openai key values ::: {openai_key}\")  # í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” print ê¸ˆì§€)\n",
    "print(f\"anthropic key values ::: {anthropic_key}\")  # í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” print ê¸ˆì§€)\n",
    "print(f\"huggingface_token::: {huggingface_token}\")  # í…ŒìŠ¤íŠ¸ìš© (ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” print ê¸ˆì§€)\n",
    "#...ddd\n",
    "\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cfc78",
   "metadata": {},
   "source": [
    "#### **0.í”„ë¡¬í”„íŠ¸ êµ¬ì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0131d4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# ëª©ì°¨\\n1. Introdoction to êµ­ê°€ê±´ê°•ë³´í—˜ ì‹¬ì‚¬ì²­êµ¬ì‹œìŠ¤í…œ\\n    1.1 í•œêµ­ ê±´ê°•ë³´í—˜ ì²´ê³„\\n        1.1.1 Governance of NHI in Korea\\n        1.1.2 HIRAâ€™s Role in 4P Concept\\n        1.1.3 Key Strategy on Korea NHI\\n\\n    1.2  ì‹¬ì‚¬í‰ê°€ì‹œìŠ¤í…œ í‘œì¤€ëª¨ë¸\\n    1.3 ì‹¬ì‚¬í‰ê°€ì •ë³´ì‹œìŠ¤í…œ Vision\\n\\n2.  Service Offering\\n    2.1 DUR System\\n        2.1.1 ê°œìš”\\n        2.1.2. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëª©ì°¨\n",
    "str = \"\"\"\n",
    "\n",
    "\n",
    "#ëª©ì°¨\n",
    "1. Introdoction to êµ­ê°€ê±´ê°•ë³´í—˜ ì‹¬ì‚¬ì²­êµ¬ì‹œìŠ¤í…œ\n",
    "    1.1 í•œêµ­ ê±´ê°•ë³´í—˜ ì²´ê³„\n",
    "        1.1.1 Governance of NHI in Korea\n",
    "        1.1.2 HIRAâ€™s Role in 4P Concept\n",
    "        1.1.3 Key Strategy on Korea NHI\n",
    "    1.2 ì‹¬ì‚¬í‰ê°€ì‹œìŠ¤í…œ í‘œì¤€ëª¨ë¸\n",
    "    1.3 ì‹¬ì‚¬í‰ê°€ì •ë³´ì‹œìŠ¤í…œ Vision\n",
    "\n",
    "2.  Service Offering\n",
    "    2.1 DUR System\n",
    "        2.1.1 ê°œìš”\n",
    "        2.1.2 íë¦„ë„\n",
    "    2.2 NHIIS System\n",
    "        2.2.1 ê°œìš”\n",
    "        2.2.2 NHIIS ë°ì´í„° ë° ì‚¬ìš©ìê·¸ë£¹\n",
    "        2.2.3 ê±´ê°•ë³´í—˜ ê¸‰ì—¬ì²­êµ¬, ì‹¬ì‚¬, ì§€ê¸‰ ì²˜ë¦¬ íë¦„\n",
    "        2.2.4 e-claim\n",
    "\n",
    "#ì§€ì‹œ\n",
    " 1.ëª©ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê±´ê°•ë³´í—˜_ë¸Œë¡œì…”.pptì„ ìƒì„±í•œë‹¤.\n",
    " 2.ë‹¹ì‹ ì€ ê±´ê°•ë³´í—˜ ì—…ë¬´ ì „ë¬¸ê°€ì´ë©° ë¬¸ì„œ ì‘ì„±ì—ë„ íƒì›”í•œ ëŠ¥ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤.\n",
    " 3.pptíŒŒì¼ì˜ ì²«ì¥ì€ ëª©ì°¨ë¡œ êµ¬ì„±í•œë‹¤.\n",
    " 4.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7373b",
   "metadata": {},
   "source": [
    "#### **1.PDFë¬¸ì„œë¡œë”© + Embedding + Vector DB êµ¬ì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4203de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì½ì–´ì˜¤ê¸°\n",
    "load_dotenv(override=True)  # .env íŒŒì¼ì„ ë®ì–´ì“°ê¸° ëª¨ë“œë¡œ ì½ê¸°\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "huggingface_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "from pdf_full_loader import PDFFullLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#PyPDFLoader ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# 1. PDF ë¡œë”© ë° ë¶„í• \n",
    "# loader = PDFFullLoader(\"sample.pdf\")\n",
    "# documents = loader.load()\n",
    "\n",
    "# loader = PyPDFLoader(r\"G:\\ë‚´ ë“œë¼ì´ë¸Œ\\LLM-RAG-LangChain\\ì œì•ˆìš”ì²­ì„œ_ë°”ë ˆì¸.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "# 2. ì„ë² ë”©\n",
    "# embedding = HuggingFaceEmbeddings(model_name=\"jhgan/ko-sroberta-multitask\")\n",
    "embedding_model = OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=openai_key)    \n",
    "\n",
    "# 3. Chroma Vector DB |  FAISS DB êµ¬ì„±\n",
    "vectordb = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db\")\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 4})\n",
    "# faiss_db = FAISS.from_documents(chunks, embedding_model)\n",
    "# retriever = faiss_db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 4. ì§ˆì˜ì‘ë‹µ\n",
    "retriever.search(\"í•œêµ­ ê±´ê°•ë³´í—˜ ì²´ê³„ì˜ ê±°ë²„ë„ŒìŠ¤ëŠ” ì–´ë–»ê²Œ êµ¬ì„±ë˜ì–´ ìˆë‚˜ìš”?\")  # ì§ˆì˜ì‘ë‹µ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314f8475",
   "metadata": {},
   "source": [
    "#### **2.LLM ì—°ê²°**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd882978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name = 'exaone-3.5-2.4b-instruct',\n",
    "    openai_api_key=\"lm-studio\",\n",
    "    openai_api_base=\"http://127.0.0.1:1234/v1\",\n",
    ")\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from anthropic import Anthropic\n",
    "import httpx  \n",
    "\n",
    "# vfy_client = httpx.Client(verify=False)\n",
    "\n",
    "# # 1. ì§ì ‘ Anthropic\n",
    "# client = Anthropic(api_key=anthropic_key, http_client=vfy_client)\n",
    "\n",
    "# # 2. Langchain Anthropic ëª¨ë¸ í˜¸ì¶œ\n",
    "# llm = ChatAnthropic(\n",
    "#     model_name =\"claude-3-opus-20240229\",\n",
    "#     anthropic_api_key=anthropic_key,)\n",
    "\n",
    "# llm._client = client\n",
    "\n",
    "# chat._client._transport._pool._ssl_context.check_hostname = False\n",
    "# chat._client._transport._pool._ssl_context.verify_mode = 0  # ssl.CERT_NONE\n",
    "# llm.invoke(\"ì•ˆë…•~ ë„ˆë¥¼ ì†Œê°œí•´ì¤„ë˜?\")\n",
    "\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=retriever,  # ì´ê±´ `vectorstore.as_retriever()`ì™€ ê°™ì€ ê°ì²´ì—¬ì•¼ í•¨\n",
    "#     return_source_documents=True\n",
    "# )\n",
    "\n",
    "# result = qa_chain.invoke({\"query\": \"ì œì•ˆìš”ì²­ì„œì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\"})\n",
    "# print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d79505",
   "metadata": {},
   "source": [
    "#### **3. LLM ê¸°ë°˜ ëª©ì°¨ ìƒì„±**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a44a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "def generate_toc(llm, sample_text):\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"content\"],\n",
    "        template=\"\"\"\n",
    "        ë‹¤ìŒ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°œí‘œìš© í”„ë ˆì  í…Œì´ì…˜ì˜ ëª©ì°¨ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n",
    "        ê° í•­ëª©ì€ í•œ ì¤„ì”© ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "        {content}\n",
    "        ëª©ì°¨ : \n",
    "        1. ì‚¬ì—… ê°œìš”\n",
    "        2. ì‚¬ì—… ëª©í‘œ\n",
    "        3. ìƒì„¸ ìš”êµ¬ì‚¬í•­     \n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    result = chain.run(sample_text[:2000])  # ì¼ë¶€ë§Œ ì‚¬ìš©\n",
    "    return [line.strip() for line in result.split(\"\\n\") if line.strip()]\n",
    "\n",
    "def callme():\n",
    "    print(\"callme function called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18357b3b",
   "metadata": {},
   "source": [
    "#### **4. ëª©ì°¨ë³„ ìì„¸í•œ ë‚´ìš© ìƒì„± (Retriever ê¸°ë°˜ RAG)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6252ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def generate_content_per_topic(llm, retriever, topics):\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "    results = {}\n",
    "    for topic in topics:\n",
    "        query = f\"í”„ë ˆì  í…Œì´ì…˜ì—ì„œ '{topic}' í•­ëª©ì„ ì„¤ëª…í•  ë‚´ìš©ì„ ìì„¸íˆ ì‘ì„±í•´ ì£¼ì„¸ìš”.\"\n",
    "        answer = qa_chain.run(query)\n",
    "        results[topic] = answer\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3803cd",
   "metadata": {},
   "source": [
    "#### **5. PPT ìƒì„± ë¡œì§**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51a8557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install python-pptx\n",
    "from pptx import Presentation\n",
    "from pptx.util import Inches\n",
    "from io import BytesIO\n",
    "\n",
    "def create_ppt(topics, topic_contents):\n",
    "    prs = Presentation()\n",
    "    for topic in topics:\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "        slide.shapes.title.text = topic\n",
    "        content = topic_contents.get(topic, \"ë‚´ìš© ì¤€ë¹„ ì¤‘\")\n",
    "        slide.placeholders[1].text = content[:1500]  # ë„ˆë¬´ ê¸¸ë©´ PPT ì—ëŸ¬\n",
    "\n",
    "    output_path = \"generated_ppt.pptx\"\n",
    "    prs.save(output_path)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3632c14",
   "metadata": {},
   "source": [
    "#### ***ì „ì²´ ì‹¤í–‰***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ë¬¸ì„œ ë¡œë”© ë° Vector DB ìƒì„±\n",
    "# loader = PDFFullLoader(r\"C:\\Users\\MOON YOUNGHO\\llm\\llm_mission\\ì œì•ˆìš”ì²­ì„œ_ë°”ë ˆì¸.pdf\",chunk_size=1000, chunk_overlap=50)\n",
    "# documents = loader.load()\n",
    "\n",
    "# embedding_model = OpenAIEmbeddings(model='text-embedding-3-small',openai_api_key=openai_key)   \n",
    "\n",
    "# vectordb = Chroma.from_documents(documents, embedding, persist_directory=\"./chroma_db\")\n",
    "# retriever = vectordb.as_retriever()\n",
    "# faiss_db = FAISS.from_documents(documents, embedding_model)\n",
    "# retriever = faiss_db.as_retriever()\n",
    "\n",
    "# 2. ëª©ì°¨ ìƒì„±\n",
    "sample_text = \"\\n\".join(doc.page_content for doc in chunks)  # ì¼ë¶€ ì‚¬ìš©\n",
    "print(sample_text)\n",
    "topics = generate_toc(llm, sample_text)\n",
    "print(topics)\n",
    "\n",
    "# 3. ê° ëª©ì°¨ë³„ ë‚´ìš© ìƒì„±\n",
    "topic_contents = generate_content_per_topic(llm, retriever, topics)\n",
    "print(\"22222222222\")\n",
    "\n",
    "# 4. PPT ìƒì„±\n",
    "ppt_path = create_ppt(topics, topic_contents)\n",
    "print(\"33333333\")\n",
    "\n",
    "print(f\"ğŸ“Š PPT ìƒì„± ì™„ë£Œ: {ppt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
